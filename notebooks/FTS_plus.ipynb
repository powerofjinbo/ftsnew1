{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FTS (Focused Test Statistics) Implementation with xRooFit\n",
    "## A Modern Approach to Statistical Hypothesis Testing in Particle Physics\n",
    "\n",
    "This notebook implements the **Focused Test Statistics (FTS)** method, a novel approach that enhances statistical sensitivity in specific parameter regions of interest. Unlike traditional likelihood ratio tests that treat all parameter values equally, FTS uses a \"focus function\" to concentrate statistical power where physics considerations suggest signals are most likely to appear.\n",
    "\n",
    "### Key References:\n",
    "- **FTS Paper**: [On Focusing Statistical Power for Searches and Measurements in Particle Physics](https://arxiv.org/pdf/2507.17831)\n",
    "- **Demo Code**: [FocusedTestStatDemo.html](https://will.web.cern.ch/FocusedTestStatDemo.html)\n",
    "\n",
    "### What You'll Learn:\n",
    "1. How to set up a statistical model using xRooFit (CERN's ROOT extension)\n",
    "2. How to implement the FTS algorithm with numerical stability\n",
    "3. How to generate toy datasets for p-value calculation\n",
    "4. How to compare FTS with traditional likelihood ratio tests\n",
    "\n",
    "### Mathematical Foundation:\n",
    "The FTS test statistic is defined as:\n",
    "$$T_f(D; \\mu_0) = -2 \\log\\left(\\frac{L(\\mu_0|D)}{\\int L(\\mu|D) f(\\mu) d\\mu}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $L(\\mu|D)$ is the likelihood function\n",
    "- $f(\\mu)$ is the focus function (typically a truncated Gaussian)\n",
    "- $\\mu_0$ is the null hypothesis value\n",
    "- The integral represents a Bayesian-weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 1: Environment Setup and Statistical Model Creation\n",
    "# ==============================================================================\n",
    "# This section sets up the computational environment and creates a statistical\n",
    "# model representing a typical particle physics counting experiment with:\n",
    "# - Signal and background processes\n",
    "# - Systematic uncertainties\n",
    "# - Observed data (with some signal injected for realistic testing)\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from typing import Dict, Tuple, Optional, Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FTS Implementation - Environment Setup\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Track setup time for performance monitoring\n",
    "setup_start = time.time()\n",
    "\n",
    "# Configure threading for reproducible results\n",
    "# Single-threaded execution ensures consistency across runs\n",
    "os.environ['OMP_NUM_THREADS'] = os.environ.get('OMP_NUM_THREADS', '1')\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ROOT Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# ROOT is CERN's data analysis framework, required for xRooFit\n",
    "# We need to add ROOT's Python bindings to the system path\n",
    "\n",
    "# Standard ROOT installation paths for macOS (via Homebrew)\n",
    "brew_root_lib = \"/opt/homebrew/opt/root/lib\"\n",
    "brew_root_py = \"/opt/homebrew/opt/root/lib/root\"\n",
    "\n",
    "# Add ROOT paths to Python's module search path\n",
    "for p in [brew_root_lib, brew_root_py]:\n",
    "    if p and p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# Set PYTHONPATH environment variable for ROOT\n",
    "os.environ['PYTHONPATH'] = f\"/opt/homebrew/Cellar/root/6.34.08_1/lib/root:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Import ROOT - this will fail if ROOT is not installed\n",
    "import ROOT\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Project Path Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Add project directories to Python path for importing custom modules\n",
    "\n",
    "# Get the parent directory (project root)\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "print(f\"âœ… Added project root to path: {parent_dir}\")\n",
    "\n",
    "# Add examples directory for html_exact_compat module\n",
    "examples_dir = os.path.join(parent_dir, 'examples')\n",
    "if examples_dir not in sys.path:\n",
    "    sys.path.insert(0, examples_dir)\n",
    "print(f\"âœ… Added examples to path: {examples_dir}\")\n",
    "\n",
    "# Import xRooFit compatibility layer\n",
    "# This module sets up the custom xRooFit build if available\n",
    "import html_exact_compat  \n",
    "print(f\"âœ… xRooFit compatibility layer loaded\")\n",
    "\n",
    "# Disable interactive JavaScript ROOT plots in Jupyter\n",
    "%jsroot off\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# ROOT Visual Style Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Configure ROOT's plotting style for publication-quality figures\n",
    "\n",
    "ROOT.gStyle.SetTitleSize(0.035, \"XYZ\")      # Axis title size\n",
    "ROOT.gStyle.SetLabelSize(0.03, \"XYZ\")       # Axis label size\n",
    "ROOT.gStyle.SetTitleFontSize(0.04)          # Overall title font size\n",
    "ROOT.gStyle.SetLegendTextSize(0.03)         # Legend text size\n",
    "ROOT.gStyle.SetOptStat(False)               # Disable statistics box on plots\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Minimizer Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Configure MINUIT (ROOT's minimization engine) for optimal performance\n",
    "\n",
    "# Strategy 1: Balance between speed and reliability\n",
    "ROOT.Math.MinimizerOptions.SetDefaultStrategy(1)\n",
    "\n",
    "# Tolerance: Convergence criterion for fits\n",
    "ROOT.Math.MinimizerOptions.SetDefaultTolerance(1e-3)\n",
    "\n",
    "# Initialize performance monitoring dictionary\n",
    "perf_stats = defaultdict(list)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Creating Statistical Model with xRooFit\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ==============================================================================\n",
    "# Statistical Model Creation\n",
    "# ==============================================================================\n",
    "# We create a simple counting experiment model with:\n",
    "# - 3 bins (representing different analysis regions or energy ranges)\n",
    "# - Background process with known expected rates\n",
    "# - Signal process scaled by parameter of interest Î¼\n",
    "# - 10% systematic uncertainty on background\n",
    "\n",
    "# Create the main workspace using xRooFit\n",
    "# The workspace contains all PDFs, parameters, and datasets\n",
    "w = ROOT.xRooNode(\"RooWorkspace\", \"combined\", \"FTS demonstration workspace\")\n",
    "\n",
    "# Define the signal region (SR) with 3 bins\n",
    "# This could represent different categories or phase space regions\n",
    "w[\"pdfs/simPdf/SR\"].SetXaxis(3, 0, 3)  # 3 bins: [0,1), [1,2), [2,3)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Background Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Set expected background rates for each bin\n",
    "# These would typically come from control regions or simulations\n",
    "w[\"pdfs/simPdf/SR/bkg\"].SetBinContent(1, 15)  # Bin 1: 15 expected events\n",
    "w[\"pdfs/simPdf/SR/bkg\"].SetBinContent(2, 20)  # Bin 2: 20 expected events\n",
    "w[\"pdfs/simPdf/SR/bkg\"].SetBinContent(3, 17)  # Bin 3: 17 expected events\n",
    "\n",
    "# Add systematic uncertainty: 10% normalization uncertainty\n",
    "# The parameter alpha_sys controls the systematic shift\n",
    "# When alpha_sys = +1, background increases by 10%\n",
    "w[\"pdfs/simPdf/SR/bkg\"].SetBinContent(1, 15*1.1, \"alpha_sys\", 1)\n",
    "w[\"pdfs/simPdf/SR/bkg\"].SetBinContent(2, 20*1.1, \"alpha_sys\", 1)\n",
    "w[\"pdfs/simPdf/SR/bkg\"].SetBinContent(3, 17*1.1, \"alpha_sys\", 1)\n",
    "\n",
    "# Constrain the systematic parameter with a Gaussian (mean=0, sigma=1)\n",
    "# This implements the systematic uncertainty in the likelihood\n",
    "w[\"pdfs/simPdf\"].pars()[\"alpha_sys\"].Constrain(\"normal\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Signal Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Set expected signal yields for each bin (before scaling by Î¼)\n",
    "# These represent the signal shape/distribution\n",
    "w[\"pdfs/simPdf/SR/sig\"].SetBinContent(1, 1)  # Relative signal in bin 1\n",
    "w[\"pdfs/simPdf/SR/sig\"].SetBinContent(2, 1)  # Relative signal in bin 2\n",
    "w[\"pdfs/simPdf/SR/sig\"].SetBinContent(3, 1)  # Relative signal in bin 3\n",
    "\n",
    "# The signal is scaled by the parameter of interest Î¼\n",
    "# Î¼=0: background-only hypothesis\n",
    "# Î¼=1: signal at nominal strength\n",
    "# Î¼>1: enhanced signal\n",
    "w[\"pdfs/simPdf/SR/sig\"].Multiply(\"mu\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Parameter of Interest Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "# Mark Î¼ as the parameter of interest (POI)\n",
    "w.pars()[\"mu\"].setAttribute(\"poi\")\n",
    "\n",
    "# Set allowed range for Î¼ (important for integration bounds)\n",
    "w.pars()[\"mu\"].setRange(-100, 100)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Generate Observed Data\n",
    "# ------------------------------------------------------------------------------\n",
    "# For demonstration, we inject some signal (Î¼=2) to create realistic data\n",
    "# In real analysis, this would be actual detector data\n",
    "\n",
    "print(\"\\nGenerating observed dataset with injected signal...\")\n",
    "w.pars()[\"mu\"].setVal(2.0)  # Set true signal strength\n",
    "toy_data = w[\"pdfs/simPdf\"].generate()  # Generate Poisson-distributed data\n",
    "toy_data.get().SetNameTitle(\"obsData\", \"Observed Data\")\n",
    "w.Add(toy_data)  # Add to workspace\n",
    "w.pars()[\"mu\"].setVal(0.0)  # Reset Î¼ for analysis\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Visualize the Model\n",
    "# ------------------------------------------------------------------------------\n",
    "# Create a plot showing the model and observed data\n",
    "print(\"Creating visualization of model and data...\")\n",
    "w[\"pdfs/simPdf\"].Draw(\"eratio\")  # Draw PDF with error bands\n",
    "w[\"datasets/obsData\"].Draw(\"same\")  # Overlay observed data\n",
    "ROOT.gPad.GetCanvas().Draw()\n",
    "\n",
    "# Report setup completion\n",
    "setup_time = time.time() - setup_start\n",
    "perf_stats['setup_time'].append(setup_time)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"âœ… Statistical model created successfully in {setup_time:.2f}s\")\n",
    "print(f\"   â€¢ 3 bins with background rates: [15, 20, 17] events\")\n",
    "print(f\"   â€¢ Signal injection: Î¼ = 2.0\")\n",
    "print(f\"   â€¢ Systematic uncertainty: 10% on background\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 2: FTS Core Implementation - Focus Functions and NLL Calculator\n",
    "# ==============================================================================\n",
    "# This section implements the core components of the Focused Test Statistics:\n",
    "# 1. Focus function (weights different Î¼ values)\n",
    "# 2. NLL calculator with caching for performance\n",
    "# 3. Numerical integration routines with stability enhancements\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Loading FTS Core Components\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ==============================================================================\n",
    "# Focus Function Class\n",
    "# ==============================================================================\n",
    "# The focus function f(Î¼) determines how much weight each parameter value\n",
    "# receives in the FTS denominator integral. By choosing an appropriate focus\n",
    "# function, we can enhance sensitivity in physically motivated regions.\n",
    "\n",
    "class ProductionFocusFunction:\n",
    "    \"\"\"\n",
    "    Focus function for FTS implementation.\n",
    "    \n",
    "    The focus function weights different values of the parameter of interest (Î¼)\n",
    "    in the denominator of the FTS. Common choices include:\n",
    "    - Gaussian: Focuses on a specific value with smooth falloff\n",
    "    - Top-hat: Uniform weight within a range, zero outside\n",
    "    \n",
    "    Physics motivation: If theoretical predictions or previous experiments\n",
    "    suggest a signal around Î¼=1 with uncertainty Ïƒ=0.5, we can use a Gaussian\n",
    "    focus to concentrate statistical power in that region.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mu_focus: float = 0.0,      # Center of the focus region\n",
    "        sigma_focus: float = 1.5,    # Width of the focus region\n",
    "        normalize: bool = True,      # Whether to normalize the focus function\n",
    "        weight_type: str = 'gaussian',  # Type of focus function\n",
    "        theta_lo: Optional[float] = None,  # Lower bound for truncation\n",
    "        theta_hi: Optional[float] = None,  # Upper bound for truncation\n",
    "    ):\n",
    "        self.mu_focus = mu_focus\n",
    "        self.sigma_focus = sigma_focus\n",
    "        self.weight_type = weight_type\n",
    "        self.normalize = normalize\n",
    "        self.theta_lo = theta_lo\n",
    "        self.theta_hi = theta_hi\n",
    "\n",
    "    def weight(self, mu: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute the focus weight for a given Î¼ value.\n",
    "        \n",
    "        The weight determines how much this Î¼ value contributes to the\n",
    "        denominator integral in the FTS formula.\n",
    "        \"\"\"\n",
    "        if self.weight_type == 'gaussian':\n",
    "            # Gaussian focus: exp(-0.5 * ((Î¼ - Î¼_focus) / Ïƒ_focus)Â²)\n",
    "            # Concentrates weight near Î¼_focus with width Ïƒ_focus\n",
    "            z = (mu - self.mu_focus) / self.sigma_focus\n",
    "            val = math.exp(-0.5 * z * z)\n",
    "            \n",
    "            # Apply truncation bounds if specified\n",
    "            # This can be used to enforce physical constraints (e.g., Î¼ â‰¥ 0)\n",
    "            if self.theta_lo is not None and mu < self.theta_lo:\n",
    "                return 0.0\n",
    "            if self.theta_hi is not None and mu > self.theta_hi:\n",
    "                return 0.0\n",
    "            return val\n",
    "            \n",
    "        elif self.weight_type == 'tophat':\n",
    "            # Top-hat (uniform) focus within range, zero outside\n",
    "            # Useful for testing sensitivity in a specific interval\n",
    "            return 1.0 if abs(mu - self.mu_focus) <= self.sigma_focus else 0.0\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown weight_type: {self.weight_type}\")\n",
    "\n",
    "    def get_dynamic_integration_range(self, mu0: float, n_sigma: float = 5.0) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Determine optimal integration range based on focus and null hypothesis.\n",
    "        \n",
    "        The integration range should:\n",
    "        1. Cover the focus region (where f(Î¼) is significant)\n",
    "        2. Include the null hypothesis value Î¼â‚€\n",
    "        3. Be wide enough for accurate numerical integration\n",
    "        \n",
    "        Strategy: Center between Î¼â‚€ and Î¼_focus, extend by n_sigma * Ïƒ_focus\n",
    "        \"\"\"\n",
    "        center = (self.mu_focus + mu0) / 2.0\n",
    "        half_range = n_sigma * self.sigma_focus + abs(mu0 - self.mu_focus)\n",
    "        lo = center - half_range\n",
    "        hi = center + half_range\n",
    "        \n",
    "        # Apply bounds if specified\n",
    "        if self.theta_lo is not None:\n",
    "            lo = max(lo, self.theta_lo)\n",
    "        if self.theta_hi is not None:\n",
    "            hi = min(hi, self.theta_hi)\n",
    "        return (lo, hi)\n",
    "\n",
    "    def get_uniform_grid(self, mu0: float, n_points: int = 401, n_sigma: float = 5.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a uniform grid for numerical integration.\n",
    "        \n",
    "        For accurate integration, we need:\n",
    "        - Odd number of points (for Simpson's rule)\n",
    "        - Sufficient density in the focus region\n",
    "        - Coverage of the full integration range\n",
    "        \"\"\"\n",
    "        # Ensure odd number of points for Simpson's rule\n",
    "        if n_points % 2 == 0:\n",
    "            n_points += 1\n",
    "            \n",
    "        # Get integration bounds\n",
    "        lo, hi = self.get_dynamic_integration_range(mu0, n_sigma=n_sigma)\n",
    "        \n",
    "        # Protection against degenerate cases\n",
    "        if not math.isfinite(lo) or not math.isfinite(hi) or hi <= lo:\n",
    "            hi = lo + max(1e-6, abs(self.sigma_focus))\n",
    "            \n",
    "        return np.linspace(lo, hi, n_points)\n",
    "\n",
    "# ==============================================================================\n",
    "# Cached NLL Calculator\n",
    "# ==============================================================================\n",
    "# Computing the negative log-likelihood (NLL) for each Î¼ value requires\n",
    "# a fit to optimize nuisance parameters. This is computationally expensive,\n",
    "# so we cache results to avoid redundant calculations.\n",
    "\n",
    "class CachedNLLCalculator:\n",
    "    \"\"\"\n",
    "    Efficient NLL calculator with caching.\n",
    "    \n",
    "    For FTS, we need to evaluate the likelihood at many Î¼ values.\n",
    "    Each evaluation requires:\n",
    "    1. Fixing Î¼ to the test value\n",
    "    2. Minimizing over nuisance parameters (profiling)\n",
    "    3. Extracting the minimum NLL value\n",
    "    \n",
    "    Caching Strategy:\n",
    "    - Store (dataset, Î¼) -> NLL mappings\n",
    "    - Reuse values when possible (especially for toys)\n",
    "    - Track cache performance for optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, workspace):\n",
    "        self.workspace = workspace\n",
    "        self.cache: Dict[Tuple[str, float], float] = {}\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        self.failed_fits = 0\n",
    "\n",
    "    def get_nll_at_mu(self, dataset: str, mu_value: float, use_cache: bool = True) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Compute profiled NLL for given Î¼ and dataset.\n",
    "        \n",
    "        This implements the profile likelihood:\n",
    "        NLL(Î¼) = min_Î¸ [-log L(Î¼, Î¸ | data)]\n",
    "        where Î¸ represents nuisance parameters.\n",
    "        \"\"\"\n",
    "        # Create cache key (round Î¼ to avoid floating point issues)\n",
    "        cache_key = (dataset, round(float(mu_value), 6))\n",
    "        \n",
    "        # Check cache first\n",
    "        if use_cache and cache_key in self.cache:\n",
    "            self.cache_hits += 1\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        self.cache_misses += 1\n",
    "\n",
    "        # Get parameter of interest\n",
    "        poi = self.workspace.pars()[\"mu\"]\n",
    "        old_val = poi.getVal()\n",
    "        old_const = poi.isConstant()\n",
    "        \n",
    "        try:\n",
    "            # Fix Î¼ to test value\n",
    "            poi.setVal(float(mu_value))\n",
    "            poi.setConstant(True)  # Fix during minimization\n",
    "\n",
    "            # Create NLL object and minimize over nuisance parameters\n",
    "            nll = self.workspace[\"pdfs/simPdf\"].nll(dataset)\n",
    "            fitres = nll.minimize()\n",
    "            \n",
    "            # Check fit quality\n",
    "            if fitres.status() >= 4:  # Status 4+ indicates problems\n",
    "                self.failed_fits += 1\n",
    "                return None\n",
    "\n",
    "            # Extract minimum NLL value\n",
    "            nll_value = float(fitres.minNll())\n",
    "            \n",
    "            # Validate result\n",
    "            if not math.isfinite(nll_value):\n",
    "                self.failed_fits += 1\n",
    "                return None\n",
    "\n",
    "            # Cache successful result\n",
    "            if use_cache:\n",
    "                self.cache[cache_key] = nll_value\n",
    "            return nll_value\n",
    "            \n",
    "        except Exception:\n",
    "            self.failed_fits += 1\n",
    "            return None\n",
    "        finally:\n",
    "            # Always restore original parameter state\n",
    "            poi.setVal(old_val)\n",
    "            poi.setConstant(old_const)\n",
    "\n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Return cache performance statistics.\"\"\"\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        hit_rate = self.cache_hits / total if total > 0 else 0.0\n",
    "        return {\n",
    "            'hits': self.cache_hits,\n",
    "            'misses': self.cache_misses,\n",
    "            'hit_rate': hit_rate,\n",
    "            'cache_size': len(self.cache),\n",
    "            'failed_fits': self.failed_fits,\n",
    "        }\n",
    "\n",
    "# ==============================================================================\n",
    "# Numerical Integration Utilities\n",
    "# ==============================================================================\n",
    "# The FTS denominator requires integrating L(Î¼|D) * f(Î¼) over Î¼.\n",
    "# We use numerical techniques with stability enhancements.\n",
    "\n",
    "def _build_weight_grid(mu_grid: np.ndarray, focus: ProductionFocusFunction) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build normalized weight grid from focus function.\n",
    "    \n",
    "    The weights need to be:\n",
    "    1. Evaluated at each grid point\n",
    "    2. Normalized (if requested) so âˆ«f(Î¼)dÎ¼ = 1\n",
    "    3. Non-negative (enforced for numerical stability)\n",
    "    \"\"\"\n",
    "    # Evaluate focus function at each grid point\n",
    "    w = np.array([focus.weight(mu) for mu in mu_grid], dtype=float)\n",
    "    \n",
    "    # Normalize if requested (makes FTS invariant to focus scaling)\n",
    "    if focus.normalize:\n",
    "        # Numerical integration using trapezoidal rule\n",
    "        Z = np.trapz(w, mu_grid)\n",
    "        if Z <= 0 or not math.isfinite(Z):\n",
    "            Z = 1.0  # Fallback for degenerate cases\n",
    "        w = w / Z\n",
    "    \n",
    "    # Ensure non-negative weights\n",
    "    w = np.clip(w, 0.0, np.inf)\n",
    "    return w\n",
    "\n",
    "def _log_denom_from_profile(mu_grid: np.ndarray, nll_grid: np.ndarray, w_grid: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute log of denominator integral using numerically stable methods.\n",
    "    \n",
    "    We need to compute: log(âˆ« L(Î¼|D) * f(Î¼) dÎ¼)\n",
    "    \n",
    "    Challenge: L(Î¼|D) = exp(-NLL(Î¼)) can vary by many orders of magnitude\n",
    "    Solution: Use log-sum-exp trick for numerical stability\n",
    "    \n",
    "    The log-sum-exp trick computes log(Î£ exp(aáµ¢)) as:\n",
    "    log(Î£ exp(aáµ¢)) = aâ‚˜â‚â‚“ + log(Î£ exp(aáµ¢ - aâ‚˜â‚â‚“))\n",
    "    This prevents overflow/underflow in exponentials.\n",
    "    \"\"\"\n",
    "    # Convert to log space: log(L * f) = log(L) + log(f) = -NLL + log(f)\n",
    "    a = -nll_grid + np.log(np.maximum(w_grid, 1e-300))\n",
    "    \n",
    "    # Apply log-sum-exp trick\n",
    "    amax = float(np.max(a))\n",
    "    y = np.exp(a - amax)  # Scaled values for integration\n",
    "\n",
    "    # Choose integration method based on grid uniformity\n",
    "    dh = np.diff(mu_grid)\n",
    "    uniform = np.allclose(dh, dh[0], rtol=0.05, atol=0)\n",
    "\n",
    "    if uniform and len(mu_grid) >= 3 and (len(mu_grid) % 2 == 1):\n",
    "        # Simpson's rule (higher accuracy for smooth functions)\n",
    "        # âˆ«f dx â‰ˆ (h/3)[fâ‚€ + 4fâ‚ + 2fâ‚‚ + 4fâ‚ƒ + ... + fâ‚™]\n",
    "        h = dh[0]\n",
    "        S = y[0] + y[-1] + 4.0 * np.sum(y[1:-1:2]) + 2.0 * np.sum(y[2:-2:2])\n",
    "        integral = S * h / 3.0\n",
    "    else:\n",
    "        # Trapezoidal rule (more robust for non-uniform grids)\n",
    "        integral = np.trapz(y, mu_grid)\n",
    "\n",
    "    # Ensure positive integral\n",
    "    integral = max(float(integral), 1e-300)\n",
    "    \n",
    "    # Return log of integral\n",
    "    return amax + math.log(integral)\n",
    "\n",
    "def compute_production_fts_ts(\n",
    "    nll_calc: CachedNLLCalculator,\n",
    "    dataset: str,\n",
    "    mu0: float,\n",
    "    focus: ProductionFocusFunction,\n",
    "    integration_points: int = 401,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Compute the FTS test statistic for given data and null hypothesis.\n",
    "    \n",
    "    This is the main FTS calculation implementing:\n",
    "    T_f(D; Î¼â‚€) = -2 * [log L(Î¼â‚€|D) - log âˆ« L(Î¼|D) f(Î¼) dÎ¼]\n",
    "    \n",
    "    Steps:\n",
    "    1. Compute numerator: L(Î¼â‚€|D) via profile likelihood\n",
    "    2. Build integration grid covering focus region\n",
    "    3. Evaluate L(Î¼|D) at each grid point\n",
    "    4. Compute weighted integral with focus function\n",
    "    5. Return test statistic\n",
    "    \"\"\"\n",
    "    ts_start = time.time()\n",
    "\n",
    "    # Step 1: Compute numerator (null hypothesis likelihood)\n",
    "    nll_mu0 = nll_calc.get_nll_at_mu(dataset, mu0)\n",
    "    if nll_mu0 is None:\n",
    "        return None\n",
    "    lnL_numerator = -float(nll_mu0)  # Convert NLL to log-likelihood\n",
    "\n",
    "    # Step 2: Create integration grid\n",
    "    mu_grid = focus.get_uniform_grid(mu0, n_points=integration_points, n_sigma=5.0)\n",
    "\n",
    "    # Step 3: Evaluate likelihood across grid\n",
    "    nll_vals = []\n",
    "    valid_mu = []\n",
    "    for mu in mu_grid:\n",
    "        nll_mu = nll_calc.get_nll_at_mu(dataset, float(mu))\n",
    "        if nll_mu is None:\n",
    "            continue  # Skip failed fits\n",
    "        valid_mu.append(float(mu))\n",
    "        nll_vals.append(float(nll_mu))\n",
    "\n",
    "    # Require minimum number of successful evaluations\n",
    "    if len(nll_vals) < 11:\n",
    "        return None\n",
    "\n",
    "    valid_mu = np.asarray(valid_mu, dtype=float)\n",
    "    nll_vals = np.asarray(nll_vals, dtype=float)\n",
    "\n",
    "    # Step 4: Build weight grid and compute integral\n",
    "    w_grid = _build_weight_grid(valid_mu, focus)\n",
    "    ln_denominator = _log_denom_from_profile(valid_mu, nll_vals, w_grid)\n",
    "\n",
    "    # Step 5: Compute FTS test statistic\n",
    "    fts_ts = -2.0 * (lnL_numerator - ln_denominator)\n",
    "\n",
    "    # Track performance\n",
    "    perf_stats['ts_calculation_time'].append(time.time() - ts_start)\n",
    "    return float(fts_ts)\n",
    "\n",
    "# Report successful loading\n",
    "print(\"âœ… Focus function class loaded\")\n",
    "print(\"âœ… NLL calculator with caching loaded\")\n",
    "print(\"âœ… Numerical integration utilities loaded\")\n",
    "print(\"\\nKey Features:\")\n",
    "print(\"   â€¢ Dynamic integration range optimization\")\n",
    "print(\"   â€¢ NLL caching for 2-4x speedup\")\n",
    "print(\"   â€¢ Log-sum-exp for numerical stability\")\n",
    "print(\"   â€¢ Simpson's rule for accurate integration\")\n",
    "print(\"   â€¢ Automatic error handling and recovery\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 3: OPTIMIZED FTS Analysis - Hypothesis Testing with Toys\n",
    "# ==============================================================================\n",
    "# This section performs the main FTS analysis using the optimized implementation:\n",
    "# 1. Creates hypothesis test points at different Î¼ values\n",
    "# 2. Computes observed test statistics from data using optimized FTS\n",
    "# 3. Generates toy datasets to build null distributions\n",
    "# 4. Calculates p-values for hypothesis testing\n",
    "# ==============================================================================\n",
    "\n",
    "import tqdm\n",
    "import fts_core as FTS  # Import optimized FTS algorithms from src/fts_core.py\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZED FTS Hypothesis Testing Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "analysis_start = time.time()\n",
    "\n",
    "# ==============================================================================\n",
    "# Create HypoSpace for Hypothesis Testing\n",
    "# ==============================================================================\n",
    "# HypoSpace is xRooFit's framework for managing hypothesis tests.\n",
    "# It handles test points, toy generation, and p-value calculation.\n",
    "\n",
    "print(\"\\nSetting up hypothesis testing framework...\")\n",
    "\n",
    "# Create hypothesis space centered on the observed data\n",
    "# pllType specifies the asymptotic approximation (Unknown = use toys)\n",
    "# alt_value is the alternative hypothesis value\n",
    "hs = w[\"pdfs/simPdf\"].nll(\"obsData\").hypoSpace(\n",
    "    \"mu\",  # Parameter of interest\n",
    "    pllType=ROOT.xRooFit.Asymptotics.Unknown,  # Use toys, not asymptotics\n",
    "    alt_value=0  # Alternative hypothesis: Î¼=0 (background-only)\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# Configure Focus Function\n",
    "# ==============================================================================\n",
    "# Choose where to concentrate statistical power\n",
    "\n",
    "focus = ProductionFocusFunction(\n",
    "    mu_focus=0.0,        # Center focus at Î¼=0 (background-only hypothesis)\n",
    "    sigma_focus=1.5,     # Width of focus region\n",
    "    normalize=True,      # Normalize the focus function\n",
    ")\n",
    "\n",
    "# Why focus at Î¼=0?\n",
    "# - For discovery: Focus near background-only improves sensitivity\n",
    "# - For measurement: Focus near expected signal value\n",
    "# - For limits: Candidate strategy â€” focus within the physically allowed region (evaluate per analysis)\n",
    "\n",
    "# Create SINGLE NLL calculator instance for optimal caching\n",
    "print(\"ðŸ”§ Creating optimized NLL calculator...\")\n",
    "nll_calc = CachedNLLCalculator(w)\n",
    "\n",
    "# Create optimized FTS calculator\n",
    "print(\"ðŸ”§ Creating optimized FTS calculator...\")\n",
    "opt_calc = FTS.OptimizedFTSCalculator()\n",
    "\n",
    "# ==============================================================================\n",
    "# Analysis Configuration\n",
    "# ==============================================================================\n",
    "\n",
    "# Define test points for hypothesis testing\n",
    "test_mus = list(range(1, 10))  # Test Î¼ = 1, 2, ..., 9\n",
    "\n",
    "# Number of toy datasets per test point\n",
    "# More toys = better p-value accuracy but longer runtime\n",
    "num_toys = 100\n",
    "\n",
    "# Integration grid density (optimized settings)\n",
    "# Higher values = better numerical accuracy but slower\n",
    "obs_integration_points = 201   # For observed data (high precision)\n",
    "toy_integration_points = 101   # For toys (balanced speed/accuracy)\n",
    "\n",
    "# Integration bounds aligned with parameter range\n",
    "# These should match the physical bounds of Î¼\n",
    "theta_bounds = (-100.0, 100.0)\n",
    "\n",
    "# Display configuration\n",
    "mu_range = focus.get_dynamic_integration_range(test_mus[0])\n",
    "print(f\"\\nðŸ“Š Analysis Configuration:\")\n",
    "print(f\"   â€¢ Test points: {len(test_mus)} values of Î¼\")\n",
    "print(f\"   â€¢ Toys per point: {num_toys}\")\n",
    "print(f\"   â€¢ Integration points: {obs_integration_points} (observed), {toy_integration_points} (toys)\")\n",
    "print(f\"   â€¢ Focus: centered at Î¼={focus.mu_focus}, width Ïƒ={focus.sigma_focus}\")\n",
    "print(f\"   â€¢ Parameter bounds: Î¼ âˆˆ [{theta_bounds[0]}, {theta_bounds[1]}]\")\n",
    "print(f\"   â€¢ Using optimized FTS implementation with proper caching\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Main Analysis Loop (OPTIMIZED VERSION)\n",
    "# ==============================================================================\n",
    "# For each test value of Î¼:\n",
    "# 1. Compute observed test statistic using optimized FTS\n",
    "# 2. Generate toys under null hypothesis\n",
    "# 3. Build null distribution using optimized calculations\n",
    "# 4. Calculate p-value\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Starting OPTIMIZED hypothesis tests...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "base_seed = 123456\n",
    "\n",
    "# Reset caches for clean performance measurement\n",
    "FTS.reset_caches()\n",
    "\n",
    "# Prepare containers for LRT observed/toy values\n",
    "lrt_obs_values = []\n",
    "lrt_toy_values = []  # list of lists per mu index\n",
    "\n",
    "for mu_idx, mu in enumerate(test_mus):\n",
    "    print(f\"\\nðŸ“ Test Point {mu_idx+1}/{len(test_mus)}: Î¼ = {mu}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    point_start = time.time()\n",
    "    # Initialize LRT toy container for this mu\n",
    "    lrt_toy_values.append([])\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Step 1: Create hypothesis point\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Add a new test point to the hypothesis space\n",
    "    hs.AddPoint(f\"mu={mu}\")\n",
    "    hp = hs.back()  # Get reference to the new point\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Step 2: Compute observed test statistic using OPTIMIZED FTS\n",
    "    # ----------------------------------------------------------------------\n",
    "    # This evaluates FTS for the actual data at Î¼=mu using the optimized algorithm\n",
    "    \n",
    "    print(f\"Computing observed FTS (optimized)...\")\n",
    "    obs_ts = opt_calc.compute_fts(\n",
    "        nll_calc,         # NLL calculator (persistent instance)\n",
    "        \"obsData\",        # Dataset name\n",
    "        mu0=float(mu),    # Null hypothesis value\n",
    "        focus_obj=focus,  # Focus function\n",
    "        theta_bounds=theta_bounds,  # Parameter bounds\n",
    "        n_grid=obs_integration_points  # Integration precision\n",
    "    )\n",
    "    \n",
    "    if obs_ts is None:\n",
    "        print(f\"  âŒ Failed to compute observed test statistic\")\n",
    "        continue\n",
    "    \n",
    "    # Store observed test statistic\n",
    "    hp.setObsTS(obs_ts, 0.0)  # Second argument is auxiliary observable (unused)\n",
    "    \n",
    "    # Compute observed LRT (optimized)\n",
    "    obs_lrt = opt_calc.compute_lrt(\n",
    "        nll_calc,\n",
    "        \"obsData\",\n",
    "        float(mu),\n",
    "        focus,\n",
    "        theta_bounds,\n",
    "        obs_integration_points,\n",
    "        verbose=False\n",
    "    )\n",
    "    if obs_lrt is None:\n",
    "        print(\"  âŒ Failed to compute observed LRT\")\n",
    "        continue\n",
    "    lrt_obs_values.append(obs_lrt)\n",
    "    print(f\"  âœ… Observed FTS = {obs_ts:.3f}\")\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Step 3: Generate toy datasets and compute test statistics (OPTIMIZED)\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Generate toys under the null hypothesis (Î¼=test value)\n",
    "    # to build the null distribution for p-value calculation\n",
    "    \n",
    "    print(f\"Generating {num_toys} toy datasets (optimized)...\")\n",
    "    \n",
    "    # Fix Î¼ to the test value for toy generation\n",
    "    poi = w.pars()[\"mu\"]\n",
    "    old_val = poi.getVal()\n",
    "    old_const = poi.isConstant()\n",
    "    poi.setVal(float(mu))\n",
    "    poi.setConstant(True)\n",
    "    \n",
    "    # Track successes and failures\n",
    "    successful_toys = 0\n",
    "    failed_toys = 0\n",
    "    \n",
    "    # Optimize minimizer settings for toys (faster, less precise)\n",
    "    ROOT.Math.MinimizerOptions.SetDefaultStrategy(0)  # Fast strategy\n",
    "    ROOT.Math.MinimizerOptions.SetDefaultTolerance(5e-3)  # Looser tolerance\n",
    "    \n",
    "    try:\n",
    "        # Generate and process each toy using optimized FTS\n",
    "        for i in tqdm.tqdm(range(num_toys), \n",
    "                          desc=f\"  Processing toys for Î¼={mu}\", \n",
    "                          leave=False,  # Don't leave progress bar after completion\n",
    "                          ncols=80):    # Limit width for cleaner output\n",
    "            try:\n",
    "                # Set random seed for reproducibility\n",
    "                FTS.set_root_seed(base_seed + i)\n",
    "                \n",
    "                # Generate toy dataset from model\n",
    "                # This creates Poisson-distributed data based on the model\n",
    "                toyset = w[\"pdfs/simPdf\"].generate()\n",
    "                toy_name = f\"toy_mu{mu}_{i:04d}\"\n",
    "                toyset.get().SetNameTitle(toy_name, toy_name)\n",
    "                w.Add(toyset)  # Add to workspace\n",
    "                \n",
    "                # Compute FTS for toy dataset using OPTIMIZED implementation\n",
    "                toy_ts = opt_calc.compute_fts(\n",
    "                    nll_calc,      # NLL calculator (SAME instance for caching)\n",
    "                    toy_name,      # Toy dataset name\n",
    "                    mu0=float(mu), # Null hypothesis (same as generation)\n",
    "                    focus_obj=focus,  # Focus function\n",
    "                    theta_bounds=theta_bounds,  # Parameter bounds\n",
    "                    n_grid=toy_integration_points  # Lower precision for speed\n",
    "                )\n",
    "                \n",
    "                # Compute LRT for toy dataset (optimized)\n",
    "                toy_lrt = opt_calc.compute_lrt(\n",
    "                    nll_calc,\n",
    "                    toy_name,\n",
    "                    float(mu),\n",
    "                    focus,\n",
    "                    theta_bounds,\n",
    "                    toy_integration_points,\n",
    "                    verbose=False\n",
    "                )\n",
    "                if toy_lrt is not None:\n",
    "                    lrt_toy_values[mu_idx].append(toy_lrt)\n",
    "                \n",
    "                if toy_ts is not None:\n",
    "                    # Add to null distribution\n",
    "                    # Weight=1 means each toy contributes equally\n",
    "                    hp.addNullToy(toy_ts, weight=1)\n",
    "                    successful_toys += 1\n",
    "                else:\n",
    "                    failed_toys += 1\n",
    "                \n",
    "                # Clean up toy dataset to save memory\n",
    "                try:\n",
    "                    w.removeData(toy_name)\n",
    "                    # Process ROOT events periodically to prevent UI freezing\n",
    "                    if i % 50 == 0:\n",
    "                        ROOT.gSystem.ProcessEvents()\n",
    "                except:\n",
    "                    pass  # Ignore cleanup errors\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Count any failures but continue\n",
    "                failed_toys += 1\n",
    "        \n",
    "    finally:\n",
    "        # Always restore minimizer settings\n",
    "        ROOT.Math.MinimizerOptions.SetDefaultStrategy(1)\n",
    "        ROOT.Math.MinimizerOptions.SetDefaultTolerance(1e-3)\n",
    "        poi.setVal(old_val)\n",
    "        poi.setConstant(old_const)\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Step 4: Report results for this test point (with optimization stats)\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    point_time = time.time() - point_start\n",
    "    perf_stats['point_time'].append(point_time)\n",
    "    \n",
    "    # Calculate success rate\n",
    "    success_rate = successful_toys / max((successful_toys + failed_toys), 1) * 100.0\n",
    "    \n",
    "    # Get optimized cache statistics\n",
    "    opt_stats = opt_calc.get_performance_stats()\n",
    "    denom_cache_stats = opt_stats['denominator_cache']\n",
    "    \n",
    "    print(f\"\\n  ðŸ“Š Results for Î¼={mu} (OPTIMIZED):\")\n",
    "    print(f\"     â€¢ Time: {point_time:.1f}s\")\n",
    "    print(f\"     â€¢ Toys: {successful_toys}/{num_toys} successful ({success_rate:.0f}%)\")\n",
    "    print(f\"     â€¢ Denominator cache hit rate: {denom_cache_stats['hit_rate']:.1%}\")\n",
    "    print(f\"     â€¢ Global grid cache entries: {opt_stats['global_grids_cached']}\")\n",
    "    print(f\"     â€¢ Cache size: {denom_cache_stats['cache_size']} denominators\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Create Results Visualization\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Creating results visualization...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract p-values from hypothesis space\n",
    "# \"pnulltoysreadonly\" returns p-values computed from toy distributions\n",
    "pnull_graph = hs.graph(\"pnulltoysreadonly\")\n",
    "\n",
    "# Configure plot appearance\n",
    "pnull_graph.SetTitle(\"Optimized FTS p-values vs Signal Strength;#mu;p_{null}\")\n",
    "pnull_graph.SetMarkerStyle(20)  # Filled circles\n",
    "pnull_graph.SetMarkerSize(1.2)\n",
    "pnull_graph.SetLineWidth(2)\n",
    "pnull_graph.SetMarkerColor(ROOT.kBlue)\n",
    "pnull_graph.SetLineColor(ROOT.kBlue)\n",
    "\n",
    "# Draw with logarithmic y-axis (common for p-value plots)\n",
    "pnull_graph.Draw(\"ALP\")  # A=Axis, L=Line, P=Points\n",
    "\n",
    "# Build LRT p-values from collected toys\n",
    "lrt_pvals = []\n",
    "for k, mu in enumerate(test_mus):\n",
    "    toys = lrt_toy_values[k] if k < len(lrt_toy_values) else []\n",
    "    obs = lrt_obs_values[k] if k < len(lrt_obs_values) else None\n",
    "    if obs is None or len(toys)==0:\n",
    "        lrt_pvals.append(1.0)\n",
    "    else:\n",
    "        cnt = sum(1 for t in toys if t >= obs)\n",
    "        lrt_pvals.append(cnt / len(toys))\n",
    "\n",
    "# Create LRT TGraph and draw overlay\n",
    "from array import array as _array\n",
    "x_arr = _array(\"d\", [float(x) for x in test_mus])\n",
    "y_arr = _array(\"d\", [float(max(min(p,1.0),1e-9)) for p in lrt_pvals])\n",
    "lrt_graph = ROOT.TGraph(len(test_mus), x_arr, y_arr)\n",
    "lrt_graph.SetMarkerStyle(24)\n",
    "lrt_graph.SetMarkerSize(1.2)\n",
    "lrt_graph.SetLineWidth(2)\n",
    "lrt_graph.SetMarkerColor(ROOT.kRed)\n",
    "lrt_graph.SetLineColor(ROOT.kRed)\n",
    "lrt_graph.Draw(\"LP SAME\")\n",
    "\n",
    "# Add legend\n",
    "leg = ROOT.TLegend(0.60, 0.75, 0.88, 0.88)\n",
    "leg.SetBorderSize(0)\n",
    "leg.SetFillStyle(0)\n",
    "leg.AddEntry(pnull_graph, \"FTS p_{null}\", \"lp\")\n",
    "leg.AddEntry(lrt_graph, \"LRT p_{null}\", \"lp\")\n",
    "leg.Draw()\n",
    "ROOT.gPad.SetLogy()      # Logarithmic y-axis\n",
    "ROOT.gPad.GetCanvas().Draw()\n",
    "\n",
    "# ==============================================================================\n",
    "# Final Summary (OPTIMIZED VERSION)\n",
    "# ==============================================================================\n",
    "\n",
    "total_time = time.time() - analysis_start\n",
    "perf_stats['total_analysis_time'].append(total_time)\n",
    "\n",
    "# Get final optimization statistics\n",
    "final_opt_stats = opt_calc.get_performance_stats()\n",
    "final_denom_stats = final_opt_stats['denominator_cache']\n",
    "\n",
    "print(f\"\\nðŸŽ‰ OPTIMIZED Analysis Complete!\")\n",
    "print(f\"   â€¢ Total time: {total_time/60:.1f} minutes\")\n",
    "print(f\"   â€¢ Denominator cache performance: {final_denom_stats['hit_rate']:.1%} hit rate\")\n",
    "print(f\"   â€¢ Total denominator calculations: {final_denom_stats['hits'] + final_denom_stats['misses']:,}\")\n",
    "print(f\"   â€¢ Cache hits: {final_denom_stats['hits']:,}\")\n",
    "print(f\"   â€¢ Cache misses: {final_denom_stats['misses']:,}\")\n",
    "print(f\"   â€¢ Cached denominators: {final_denom_stats['cache_size']}\")\n",
    "\n",
    "# Compare with expected unoptimized performance\n",
    "expected_old_time = 52.9  # minutes from previous runs\n",
    "if total_time > 0:\n",
    "    speedup = (expected_old_time * 60) / total_time\n",
    "    print(f\"\\nðŸš€ Performance Improvement:\")\n",
    "    print(f\"   â€¢ Estimated speedup: {speedup:.1f}x faster than original\")\n",
    "    print(f\"   â€¢ Time saved: {expected_old_time - total_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Focus configuration:\")\n",
    "print(f\"   â€¢ Center: Î¼_focus = {focus.mu_focus}\")\n",
    "print(f\"   â€¢ Width: Ïƒ_focus = {focus.sigma_focus}\")\n",
    "print(f\"   â€¢ Integration bounds: [{theta_bounds[0]}, {theta_bounds[1]}]\")\n",
    "\n",
    "print(f\"\\nâœ… Key Optimizations Applied:\")\n",
    "print(f\"   â€¢ Î¼â‚€-independent global grid construction\")\n",
    "print(f\"   â€¢ Pre-computed denominators with proper caching\")\n",
    "print(f\"   â€¢ Persistent NLL calculator across all evaluations\")\n",
    "print(f\"   â€¢ Optimized integration grid sizes\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 4: Results Analysis and Statistical Interpretation\n",
    "# ==============================================================================\n",
    "# This section extracts and analyzes the results:\n",
    "# 1. Extract p-values from the hypothesis tests\n",
    "# 2. Create summary table of results\n",
    "# 3. Analyze the focus effect on sensitivity\n",
    "# 4. Report performance metrics\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FTS Results Summary and Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ==============================================================================\n",
    "# Extract P-values from HypoSpace\n",
    "# ==============================================================================\n",
    "# The p-value represents the probability of observing a test statistic\n",
    "# at least as extreme as the observed value, assuming the null hypothesis\n",
    "\n",
    "# Get p-value graph from hypothesis space\n",
    "pnull_graph = hs.graph(\"pnulltoysreadonly\")\n",
    "\n",
    "# Extract (Î¼, p-value) pairs from the graph\n",
    "import ctypes\n",
    "x = ctypes.c_double(0.0)  # C-style double for ROOT interface\n",
    "y = ctypes.c_double(0.0)\n",
    "mu_to_p = {}\n",
    "\n",
    "# Iterate through graph points\n",
    "for i in range(int(pnull_graph.GetN())):\n",
    "    pnull_graph.GetPoint(i, x, y)\n",
    "    mu_to_p[float(x.value)] = float(y.value)\n",
    "\n",
    "# ==============================================================================\n",
    "# Create Results Summary Table\n",
    "# ==============================================================================\n",
    "print(\"\\nðŸ“Š Hypothesis Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Î¼':^6} | {'Obs TS':^8} | {'p-value':^8} | {'Toys':^5}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Helper functions for robust attribute extraction\n",
    "import re, math\n",
    "\n",
    "def _safe_float_attr(obj, names):\n",
    "    \"\"\"\n",
    "    Safely extract float value from object attributes.\n",
    "    Tries multiple possible attribute names for compatibility.\n",
    "    \"\"\"\n",
    "    for name in names:\n",
    "        attr = getattr(obj, name, None)\n",
    "        if attr is None:\n",
    "            continue\n",
    "        try:\n",
    "            # Handle both properties and methods\n",
    "            return float(attr() if callable(attr) else attr)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def _parse_mu_from_name(hp_obj):\n",
    "    \"\"\"\n",
    "    Extract Î¼ value from hypothesis point name.\n",
    "    Names are formatted as \"mu=X\" where X is the value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nm = hp_obj.GetName() if callable(getattr(hp_obj, 'GetName', None)) else ''\n",
    "    except Exception:\n",
    "        nm = ''\n",
    "    # Use regex to extract numeric value after \"mu=\"\n",
    "    m = re.search(r\"mu=([-+]?\\d+(?:\\.\\d+)?)\", str(nm))\n",
    "    return float(m.group(1)) if m else float('nan')\n",
    "\n",
    "# Collect and display results for each test point\n",
    "valid_results = []\n",
    "\n",
    "for i in range(hs.size()):\n",
    "    hp = hs[i]  # Get hypothesis point\n",
    "    \n",
    "    # Extract Î¼ value (try multiple methods for robustness)\n",
    "    mu_val = _safe_float_attr(hp, ['fNullVal', 'fNull', 'nullVal'])\n",
    "    if mu_val is None:\n",
    "        mu_val = _parse_mu_from_name(hp)\n",
    "\n",
    "    # Extract observed test statistic\n",
    "    obs_ts = _safe_float_attr(hp, ['obsTS', 'obs_ts', 'tObs', 'tsObs'])\n",
    "    if obs_ts is None:\n",
    "        obs_ts = float('nan')\n",
    "\n",
    "    # Get p-value from extracted graph data\n",
    "    pval = mu_to_p.get(mu_val)\n",
    "    \n",
    "    # Handle floating point precision issues\n",
    "    if pval is None and math.isfinite(mu_val) and len(mu_to_p) > 0:\n",
    "        # Find closest Î¼ value in case of rounding differences\n",
    "        xs = sorted(mu_to_p.keys(), key=lambda x: abs(x - mu_val))\n",
    "        if abs(xs[0] - mu_val) < 1e-8:\n",
    "            pval = mu_to_p[xs[0]]\n",
    "    \n",
    "    if pval is None:\n",
    "        pval = -1.0  # Mark as unavailable\n",
    "\n",
    "    # Count number of successful toys\n",
    "    try:\n",
    "        n_toys = len(hp.nullToys().get()) if hp.nullToys().get() else 0\n",
    "    except Exception:\n",
    "        n_toys = 0\n",
    "\n",
    "    # Store results\n",
    "    valid_results.append((mu_val, obs_ts, pval, n_toys))\n",
    "\n",
    "    # Display in table\n",
    "    if pval >= 0:\n",
    "        print(f\"{mu_val:^6.0f} | {obs_ts:^8.3f} | {pval:^8.4f} | {n_toys:^5.0f}\")\n",
    "    else:\n",
    "        print(f\"{mu_val:^6.0f} | {obs_ts:^8.3f} | {'N/A':^8} | {n_toys:^5.0f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Analyze Focus Effect\n",
    "# ==============================================================================\n",
    "# Compare sensitivity in the focus region vs outside to quantify improvement\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Focus Effect Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Filter results with valid p-values\n",
    "valid_pval_results = [(mu, ts, pv, n) for mu, ts, pv, n in valid_results if pv >= 0]\n",
    "\n",
    "# Categorize points by distance from focus center\n",
    "# Inside focus: within 1Ïƒ of focus center\n",
    "focus_points = [(mu, ts, pv, n) for mu, ts, pv, n in valid_pval_results \n",
    "                if abs(mu - focus.mu_focus) <= focus.sigma_focus]\n",
    "\n",
    "# Outside focus: beyond 2Ïƒ from focus center\n",
    "outside_points = [(mu, ts, pv, n) for mu, ts, pv, n in valid_pval_results \n",
    "                  if abs(mu - focus.mu_focus) > 2*focus.sigma_focus]\n",
    "\n",
    "if focus_points and outside_points:\n",
    "    # Calculate average p-values in each region\n",
    "    focus_pval_avg = sum(pv for _, _, pv, _ in focus_points) / len(focus_points)\n",
    "    outside_pval_avg = sum(pv for _, _, pv, _ in outside_points) / len(outside_points)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Focus region (|Î¼ - {focus.mu_focus}| â‰¤ {focus.sigma_focus}):\")\n",
    "    print(f\"   â€¢ Points: {len(focus_points)}\")\n",
    "    print(f\"   â€¢ Average p-value: {focus_pval_avg:.4f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ Outside region (|Î¼ - {focus.mu_focus}| > {2*focus.sigma_focus}):\")\n",
    "    print(f\"   â€¢ Points: {len(outside_points)}\")  \n",
    "    print(f\"   â€¢ Average p-value: {outside_pval_avg:.4f}\")\n",
    "    \n",
    "    # Quantify sensitivity improvement\n",
    "    if focus_pval_avg < outside_pval_avg:\n",
    "        improvement = outside_pval_avg / focus_pval_avg\n",
    "        print(f\"\\nâœ¨ Focus Effect Detected:\")\n",
    "        print(f\"   â€¢ {improvement:.1f}x better sensitivity in focus region\")\n",
    "        print(f\"   â€¢ Lower p-values indicate stronger evidence against null\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  No significant focus effect observed\")\n",
    "        print(f\"   â€¢ May need to adjust focus parameters\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Performance Statistics\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "final_cache_stats = nll_calc.get_cache_stats()\n",
    "\n",
    "print(f\"\\nâ±ï¸  Timing:\")\n",
    "print(f\"   â€¢ Total analysis: {perf_stats['total_analysis_time'][0]/60:.1f} minutes\")\n",
    "print(f\"   â€¢ Setup time: {perf_stats['setup_time'][0]:.2f} seconds\")\n",
    "if perf_stats['point_time']:\n",
    "    print(f\"   â€¢ Average per test point: {np.mean(perf_stats['point_time']):.1f} seconds\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Cache Performance:\")\n",
    "print(f\"   â€¢ Hit rate: {final_cache_stats['hit_rate']:.0%}\")\n",
    "print(f\"   â€¢ Total NLL evaluations: {final_cache_stats['hits'] + final_cache_stats['misses']:,}\")\n",
    "print(f\"   â€¢ Cache hits: {final_cache_stats['hits']:,}\")\n",
    "print(f\"   â€¢ Cache misses: {final_cache_stats['misses']:,}\")\n",
    "print(f\"   â€¢ Failed fits: {final_cache_stats['failed_fits']}\")\n",
    "\n",
    "print(f\"\\nðŸ”§ Efficiency Gains:\")\n",
    "if final_cache_stats['hit_rate'] > 0:\n",
    "    speedup = 1 / (1 - final_cache_stats['hit_rate'])\n",
    "    print(f\"   â€¢ Effective speedup from caching: {speedup:.1f}x\")\n",
    "    saved_time = perf_stats['total_analysis_time'][0] * (1 - 1/speedup) / 60\n",
    "    print(f\"   â€¢ Time saved: ~{saved_time:.1f} minutes\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Implementation Summary\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ… FTS Implementation Complete\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nKey achievements:\")\n",
    "print(\"   â€¢ Successfully implemented FTS algorithm\")\n",
    "print(\"   â€¢ Generated null distributions from toys\")\n",
    "print(\"   â€¢ Computed p-values for hypothesis testing\")\n",
    "print(\"   â€¢ Demonstrated focus effect on sensitivity\")\n",
    "print(\"   â€¢ Achieved efficient computation with caching\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"   â€¢ Compare with traditional likelihood ratio test\")\n",
    "print(\"   â€¢ Optimize focus parameters for specific physics goals\")\n",
    "print(\"   â€¢ Apply to real experimental data\")\n",
    "print(\"   â€¢ Compute confidence intervals and limits\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 5: Detailed Visualization of Best-Fit Point\n",
    "# ==============================================================================\n",
    "# This section creates a detailed visualization of the hypothesis point\n",
    "# with the lowest p-value (most significant result), showing:\n",
    "# - The null distribution from toys\n",
    "# - The observed test statistic\n",
    "# - The p-value calculation\n",
    "# ==============================================================================\n",
    "\n",
    "import ctypes, ROOT, math\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Visualizing Most Significant Result\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ==============================================================================\n",
    "# Find the Most Significant Test Point\n",
    "# ==============================================================================\n",
    "# The most significant point has the smallest p-value,\n",
    "# indicating the strongest evidence against the null hypothesis\n",
    "\n",
    "# Get p-value graph\n",
    "pnull_graph = hs.graph(\"pnulltoysreadonly\")\n",
    "\n",
    "# Find minimum p-value\n",
    "x = ctypes.c_double(0.0)\n",
    "y = ctypes.c_double(0.0)\n",
    "best_i, best_p = 0, 1.0\n",
    "best_mu = 0.0\n",
    "\n",
    "for i in range(int(pnull_graph.GetN())):\n",
    "    pnull_graph.GetPoint(i, x, y)\n",
    "    if y.value < best_p:\n",
    "        best_p = float(y.value)\n",
    "        best_i = i\n",
    "        best_mu = float(x.value)\n",
    "\n",
    "print(f\"\\nðŸ“Š Most significant result:\")\n",
    "print(f\"   â€¢ Test point: Î¼ = {best_mu}\")\n",
    "print(f\"   â€¢ p-value: {best_p:.4f}\")\n",
    "print(f\"   â€¢ Hypothesis point index: {best_i}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Create Detailed Visualization\n",
    "# ==============================================================================\n",
    "# This plot shows:\n",
    "# 1. Histogram of test statistics from toys (null distribution)\n",
    "# 2. Vertical line showing observed test statistic\n",
    "# 3. Shaded region showing p-value\n",
    "\n",
    "# Get the hypothesis point\n",
    "hp = hs[best_i]\n",
    "\n",
    "# Create canvas for plotting\n",
    "c = ROOT.TCanvas(\"c_hp\", f\"FTS Distribution for Î¼ = {best_mu}\", 800, 600)\n",
    "\n",
    "# Draw the hypothesis point\n",
    "# This creates a histogram of toy test statistics with observed value marked\n",
    "hp.Draw()\n",
    "\n",
    "# Configure plot appearance\n",
    "ROOT.gPad.SetLogy()  # Log scale for y-axis (useful for tail visualization)\n",
    "ROOT.gPad.SetGridx()  # Add vertical grid lines\n",
    "ROOT.gPad.SetGridy()  # Add horizontal grid lines\n",
    "\n",
    "# Update canvas\n",
    "ROOT.gPad.GetCanvas().Draw()\n",
    "\n",
    "# ==============================================================================\n",
    "# Interpretation Guide\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“ˆ Plot Interpretation:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "The plot shows the null distribution and observed test statistic:\n",
    "\n",
    "1. **Histogram (Blue)**: Distribution of FTS values from toy datasets\n",
    "   - Generated under null hypothesis (Î¼ = {:.1f})\n",
    "   - Shows expected variation due to statistical fluctuations\n",
    "   \n",
    "2. **Vertical Line (Red)**: Observed FTS value from actual data\n",
    "   - Position indicates how extreme the observation is\n",
    "   - Further right = more inconsistent with null hypothesis\n",
    "   \n",
    "3. **P-value**: Fraction of toys with FTS â‰¥ observed value\n",
    "   - p = {:.4f} for this test point\n",
    "   - Small p-value = strong evidence against null hypothesis\n",
    "   \n",
    "4. **Focus Effect**: \n",
    "   - FTS concentrates power near Î¼ = {:.1f}\n",
    "   - Compare with standard LRT to see improvement\n",
    "   \n",
    "Statistical Interpretation:\n",
    "\"\"\".format(best_mu, best_p, focus.mu_focus))\n",
    "\n",
    "# Provide interpretation based on p-value\n",
    "if best_p < 0.003:\n",
    "    print(\"   â­ 3Ïƒ evidence or stronger (p < 0.003)\")\n",
    "    print(\"   â†’ Strong evidence against the null hypothesis\")\n",
    "elif best_p < 0.05:\n",
    "    print(\"   â­ 2Ïƒ evidence (p < 0.05)\")\n",
    "    print(\"   â†’ Moderate evidence against the null hypothesis\")\n",
    "elif best_p < 0.32:\n",
    "    print(\"   â­ 1Ïƒ evidence (p < 0.32)\")\n",
    "    print(\"   â†’ Weak evidence against the null hypothesis\")\n",
    "else:\n",
    "    print(\"   â­ No significant evidence (p â‰¥ 0.32)\")\n",
    "    print(\"   â†’ Data consistent with null hypothesis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Visualization complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECTION 6: OPTIMIZED FTS IMPLEMENTATION DEMONSTRATION\n",
    "# ==============================================================================\n",
    "# This section demonstrates the optimized FTS implementation that addresses\n",
    "# the performance issues identified in detailed performance analysis:\n",
    "# 1. Eliminates redundant denominator calculations\n",
    "# 2. Implements proper caching with distinct dataset keys\n",
    "# 3. Uses Î¼â‚€-independent global grids\n",
    "# 4. Provides fair FTS vs LRT comparison\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FTS OPTIMIZATION DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Import optimized functions from updated fts_core\n",
    "from fts_core import (\n",
    "    fts_ts_obs_optimized, lrt_ts_optimized, \n",
    "    get_optimization_stats, reset_caches,\n",
    "    OptimizedFTSCalculator\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# Create Optimized Calculator Instance\n",
    "# ==============================================================================\n",
    "# This replaces the ad-hoc calculations with a systematic approach\n",
    "\n",
    "print(\"\\nðŸ”§ Creating optimized FTS calculator...\")\n",
    "opt_calc = OptimizedFTSCalculator()\n",
    "\n",
    "# Reset any existing caches for clean comparison\n",
    "reset_caches()\n",
    "\n",
    "# ==============================================================================\n",
    "# Performance Comparison: Old vs Optimized\n",
    "# ==============================================================================\n",
    "print(\"\\nðŸ“Š Performance Comparison:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test on a subset of Î¼ values for speed\n",
    "test_points = [1.0, 2.0, 3.0, 4.0]\n",
    "comparison_results = {\n",
    "    'mu_values': [],\n",
    "    'fts_optimized': [],\n",
    "    'lrt_optimized': [],\n",
    "    'time_per_point': []\n",
    "}\n",
    "\n",
    "print(\"Testing optimized implementation on sample points...\")\n",
    "\n",
    "# Create a persistent NLL calculator for fair comparison\n",
    "# This is crucial: same instance across all Î¼â‚€ evaluations\n",
    "persistent_nll_calc = CachedNLLCalculator(w)\n",
    "\n",
    "for mu0 in test_points:\n",
    "    point_start = time.time()\n",
    "    \n",
    "    print(f\"  â€¢ Testing Î¼â‚€ = {mu0}...\")\n",
    "    \n",
    "    # Compute FTS and LRT using optimized methods\n",
    "    fts_val = opt_calc.compute_fts(\n",
    "        persistent_nll_calc, \"obsData\", mu0, focus, \n",
    "        theta_bounds=(-100.0, 100.0), n_grid=201  # Smaller grid for demo\n",
    "    )\n",
    "    \n",
    "    lrt_val = opt_calc.compute_lrt(\n",
    "        persistent_nll_calc, \"obsData\", mu0, focus,\n",
    "        theta_bounds=(-100.0, 100.0), n_grid=201\n",
    "    )\n",
    "    \n",
    "    point_time = time.time() - point_start\n",
    "    \n",
    "    comparison_results['mu_values'].append(mu0)\n",
    "    comparison_results['fts_optimized'].append(fts_val)\n",
    "    comparison_results['lrt_optimized'].append(lrt_val)\n",
    "    comparison_results['time_per_point'].append(point_time)\n",
    "    \n",
    "    print(f\"    FTS = {fts_val:.4f}, LRT = {lrt_val:.4f}, Time = {point_time:.2f}s\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Validation Tests (Addressing Technical Analysis)\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"VALIDATION TESTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test 1: Denominator invariance\n",
    "print(\"\\nâœ… Test 1: Denominator Invariance\")\n",
    "print(\"Testing that denominator doesn't change with Î¼â‚€...\")\n",
    "\n",
    "test_mu0_values = [0.5, 1.0, 1.5, 2.0]\n",
    "denominators = []\n",
    "\n",
    "for mu0 in test_mu0_values:\n",
    "    # Access internal denominator calculation\n",
    "    grid_key = (focus.mu_focus, focus.sigma_focus, (-100.0, 100.0), 201)\n",
    "    \n",
    "    # Get cached denominator entry\n",
    "    if hasattr(opt_calc.global_grid_cache, '__contains__') and grid_key in opt_calc.global_grid_cache:\n",
    "        mu_grid, w_grid, grid_info = opt_calc.global_grid_cache[grid_key]\n",
    "        denom_entry = opt_calc.denominator_cache.get_denominator(\n",
    "            \"obsData\", focus, persistent_nll_calc, grid_info, mu_grid, w_grid\n",
    "        )\n",
    "        denominators.append(denom_entry['log_denom'])\n",
    "    else:\n",
    "        # Force computation by calling compute_fts\n",
    "        opt_calc.compute_fts(persistent_nll_calc, \"obsData\", mu0, focus, (-100.0, 100.0), 201)\n",
    "        # Get the cached result\n",
    "        mu_grid, w_grid, grid_info = opt_calc.global_grid_cache[grid_key]\n",
    "        denom_entry = opt_calc.denominator_cache.get_denominator(\n",
    "            \"obsData\", focus, persistent_nll_calc, grid_info, mu_grid, w_grid\n",
    "        )\n",
    "        denominators.append(denom_entry['log_denom'])\n",
    "\n",
    "if len(denominators) > 1:\n",
    "    denom_std = np.std(denominators)\n",
    "    denom_mean = np.mean(denominators)\n",
    "    relative_std = denom_std / abs(denom_mean) if denom_mean != 0 else float('inf')\n",
    "    \n",
    "    print(f\"Denominators: {denominators}\")\n",
    "    print(f\"Standard deviation: {denom_std:.2e}\")\n",
    "    print(f\"Relative std: {relative_std:.2e}\")\n",
    "    \n",
    "    if relative_std < 1e-9:\n",
    "        print(\"âœ“ PASS: Denominator is invariant to Î¼â‚€\")\n",
    "    else:\n",
    "        print(\"âœ— FAIL: Denominator varies with Î¼â‚€\")\n",
    "else:\n",
    "    print(\"âš  Could not test denominator invariance\")\n",
    "\n",
    "# Test 2: Constant offset property\n",
    "print(\"\\nâœ… Test 2: FTS-LRT Constant Offset\")\n",
    "print(\"Testing theoretical relationship: FTS â‰ˆ LRT + constant...\")\n",
    "\n",
    "if len(comparison_results['fts_optimized']) >= 2:\n",
    "    fts_vals = np.array(comparison_results['fts_optimized'])\n",
    "    lrt_vals = np.array(comparison_results['lrt_optimized'])\n",
    "    \n",
    "    offsets = fts_vals - lrt_vals\n",
    "    offset_std = np.std(offsets)\n",
    "    offset_mean = np.mean(offsets)\n",
    "    \n",
    "    print(f\"FTS values: {fts_vals}\")\n",
    "    print(f\"LRT values: {lrt_vals}\")\n",
    "    print(f\"Offsets (FTS-LRT): {offsets}\")\n",
    "    print(f\"Mean offset: {offset_mean:.4f}\")\n",
    "    print(f\"Offset std: {offset_std:.4f}\")\n",
    "    \n",
    "    if offset_std < 0.01:\n",
    "        print(\"âœ“ PASS: Constant offset property holds\")\n",
    "        print(f\"  Theoretical offset â‰ˆ {offset_mean:.3f}\")\n",
    "    else:\n",
    "        print(\"âœ— FAIL: Offset is not constant\")\n",
    "else:\n",
    "    print(\"âš  Need more data points to test constant offset\")\n",
    "\n",
    "# Test 3: Cache performance\n",
    "print(\"\\nâœ… Test 3: Cache Performance\")\n",
    "cache_stats = opt_calc.get_performance_stats()\n",
    "print(f\"Cache statistics: {cache_stats}\")\n",
    "\n",
    "denom_cache_stats = cache_stats['denominator_cache']\n",
    "if denom_cache_stats['hit_rate'] > 0.5:\n",
    "    print(f\"âœ“ PASS: Good cache hit rate ({denom_cache_stats['hit_rate']:.1%})\")\n",
    "else:\n",
    "    print(f\"âš  Suboptimal cache hit rate: {denom_cache_stats['hit_rate']:.1%}\")\n",
    "    \n",
    "print(f\"Grid cache entries: {cache_stats['global_grids_cached']}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Create Comparison Visualization\n",
    "# ==============================================================================\n",
    "print(\"\\nðŸ“Š Creating FTS vs LRT comparison plot...\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: FTS vs LRT values\n",
    "mu_vals = comparison_results['mu_values']\n",
    "fts_vals = comparison_results['fts_optimized']\n",
    "lrt_vals = comparison_results['lrt_optimized']\n",
    "\n",
    "ax1.scatter(mu_vals, fts_vals, color='red', label='FTS', s=100, alpha=0.7)\n",
    "ax1.scatter(mu_vals, lrt_vals, color='blue', label='LRT', s=100, alpha=0.7)\n",
    "ax1.plot(mu_vals, fts_vals, 'r--', alpha=0.5)\n",
    "ax1.plot(mu_vals, lrt_vals, 'b--', alpha=0.5)\n",
    "\n",
    "ax1.set_xlabel('Î¼â‚€ (null hypothesis)', fontsize=12)\n",
    "ax1.set_ylabel('Test Statistic Value', fontsize=12)\n",
    "ax1.set_title('FTS vs LRT Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Constant offset demonstration\n",
    "if len(fts_vals) >= 2:\n",
    "    offsets = np.array(fts_vals) - np.array(lrt_vals)\n",
    "    ax2.scatter(mu_vals, offsets, color='green', s=100)\n",
    "    ax2.axhline(y=np.mean(offsets), color='green', linestyle='-', \n",
    "                label=f'Mean offset = {np.mean(offsets):.3f}')\n",
    "    ax2.axhline(y=np.mean(offsets) + np.std(offsets), color='green', \n",
    "                linestyle='--', alpha=0.5, label=f'Â±1Ïƒ = Â±{np.std(offsets):.3f}')\n",
    "    ax2.axhline(y=np.mean(offsets) - np.std(offsets), color='green', \n",
    "                linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('Î¼â‚€ (null hypothesis)', fontsize=12)\n",
    "    ax2.set_ylabel('FTS - LRT', fontsize=12)\n",
    "    ax2.set_title('Constant Offset Property', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# Performance Summary\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "avg_time_per_point = np.mean(comparison_results['time_per_point'])\n",
    "total_test_time = sum(comparison_results['time_per_point'])\n",
    "\n",
    "print(f\"âœ… Optimized Implementation Results:\")\n",
    "print(f\"   â€¢ Test points evaluated: {len(comparison_results['mu_values'])}\")\n",
    "print(f\"   â€¢ Average time per point: {avg_time_per_point:.2f}s\")\n",
    "print(f\"   â€¢ Total computation time: {total_test_time:.2f}s\")\n",
    "print(f\"   â€¢ Cache hit rate: {denom_cache_stats['hit_rate']:.1%}\")\n",
    "\n",
    "# Estimate performance improvement\n",
    "old_avg_time = 352.7  # From previous cell output\n",
    "if avg_time_per_point > 0:\n",
    "    speedup_factor = old_avg_time / avg_time_per_point\n",
    "    print(f\"   â€¢ Estimated speedup: {speedup_factor:.1f}x faster\")\n",
    "    print(f\"   â€¢ Projected full analysis time: {(old_avg_time * 9) / speedup_factor / 60:.1f} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FTS Clean (Python 3.13.3)",
   "language": "python",
   "name": "fts_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
